# -*- coding: utf-8 -*-
"""GeneticAlgoNotebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_YcVSFlhCKXszE3PfVdyYYlSyp8dpw9_
"""

import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

!pip install chemparse
import chemparse

!pip install mpltern

import mpltern

"""#Read in Dfs"""

big_df=pd.read_csv("drive/MyDrive/Lehigh_Drive/Larger Sample for ML 2.csv")
solution_energies_df = big_df[big_df["Delta H"].notnull()][["Formula","Delta H","Pressure","Temperature"]]

solution_energies_df = solution_energies_df.rename(columns={"Pressure":"pressure","Temperature":"temperature"})
solution_energies_df.describe()

solution_energies_df["Formula"].nunique()

ss_dataset = pd.read_csv("drive/MyDrive/Lehigh_Drive/solid_solution_hydrogen.csv")

ss_dataset = ss_dataset.drop(columns = ["source","Unnamed: 8"])
ss_dataset = ss_dataset[ss_dataset["phase"]!="C14 Laves"]
ss_dataset = ss_dataset.drop_duplicates(["composition_formula"])
ss_dataset.describe()

element_properties_df = pd.read_csv("drive/MyDrive/Lehigh_Drive/element_properties_4.csv")

element_properties_df = element_properties_df.set_index("Element Name")

element_properties_df = element_properties_df.drop(columns="Unnamed: 0")
element_properties_df

elastic_df = pd.read_csv("drive/MyDrive/Lehigh_Drive/elastic_prediction.csv")
elastic_df = elastic_df[["Alloy","B"]]
elastic_df

phases = pd.read_csv("drive/MyDrive/Lehigh_Drive/phase_prediction.csv")
phases = phases.drop(columns=["raw_phase"])
phases

"""#Featurize Dfs"""

def get_wstd(vals, weights):
    var = 0

    for i in range(len(vals)):
        var += (vals[i] - np.average(vals, weights = weights))**2 * weights[i]/sum(weights)

    return (var)

def featurize_df(df, composition_column):

  ss_dataset = df.copy()



  for property in element_properties_df.columns:


    addmean = []
    addvarience = []
    addmin = []
    addmax = []
    for composition in ss_dataset[composition_column]:

      parsed_comp = chemparse.parse_formula(composition)

      if parsed_comp:
        miniweights = []
        minivalues = []

        for key in parsed_comp.keys():
          if float(parsed_comp[key]) > 0:
            minivalues.append(float(element_properties_df.loc[key][property]))
            miniweights.append(float(parsed_comp[key]))
        try:
          addmean.append(np.average(minivalues,weights=miniweights))
          addvarience.append(get_wstd(minivalues, miniweights))
        except:
          print(minivalues)
          print(miniweights)
          print(composition)

        addmin.append(np.min(minivalues))
        addmax.append(np.max(minivalues))

      else:
        print(composition)
        ss_dataset = ss_dataset[ss_dataset[composition_column] != composition]

    ss_dataset["mean"+property] = addmean
    ss_dataset["variance"+property] = addvarience
    ss_dataset["min"+property] = addmin
    ss_dataset["max"+property] = addmax


  packing_fractions = []

  for formula in ss_dataset[composition_column]:
    thatrow = ss_dataset[ss_dataset[composition_column]==formula].iloc[0]



    packing_fractions.append(float(thatrow["meanCovalent Sphere Volume"]*6.02*10**2/thatrow["meanAtomic Weight"]*thatrow["meanDensity"]))


  ss_dataset["Packing Fraction"] = packing_fractions

  Smix = []

  for formula in ss_dataset[composition_column]:
    parsed_comp = chemparse.parse_formula(formula)

    smix_total = 0

    for key in parsed_comp.values():
      smix_total += (key/np.sum(list(parsed_comp.values())))*np.log(key/np.sum(list(parsed_comp.values())))

    Smix.append(smix_total)

  ss_dataset["Smix"] = Smix

  return ss_dataset.drop(columns = ["meanCovalent Sphere Volume","minCovalent Sphere Volume","maxCovalent Sphere Volume","varianceCovalent Sphere Volume"])

ss_dataset = featurize_df(ss_dataset, "composition_formula")
ss_dataset.index = pd.RangeIndex(len(ss_dataset.index))
ss_dataset.describe()

phases = featurize_df(phases,"Alloy ")

elastic_df = featurize_df(elastic_df,"Alloy")

elastic_df.describe()

"""#Elastic Prediction

"""

from sklearn.neighbors import KNeighborsRegressor

def impute_knn(hydrogen):
        ldf = hydrogen.select_dtypes(include=[np.number])
        ldf_putaside = hydrogen.select_dtypes(exclude=[np.number])

        cols_nan = ldf.columns[ldf.isna().any()].tolist()
        cols_no_nan = ldf.columns.difference(cols_nan).values

        for col in cols_nan:
            imp_test = ldf[ldf[col].isna()]
            imp_train = ldf.dropna()
            model = KNeighborsRegressor(n_neighbors=5)  # KNR Unsupervised Approach
            knr = model.fit(imp_train[cols_no_nan], imp_train[col])
            ldf.loc[hydrogen[col].isna(), col] = knr.predict(imp_test[cols_no_nan])

        return pd.concat([ldf,ldf_putaside],axis=1)

#from sklearn.model_selection import train_test_split
#b_X_train, b_X_test, b_y_train, b_y_test = train_test_split(elastic_df.drop(columns=["Alloy","B"]), elastic_df["B"], test_size = 0.25, random_state = 42)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import KBinsDiscretizer

# Bin the continuous target for stratification
n_bins = 5  # Choose number of bins based on dataset size
binned_B = pd.qcut(elastic_df["B"], q=n_bins, duplicates='drop')

# Then apply stratified train_test_split
b_X_train, b_X_test, b_y_train, b_y_test = train_test_split(
    elastic_df.drop(columns=["Alloy", "B"]),
    elastic_df["B"],
    test_size=0.25,
    random_state=42,
    stratify=binned_B
)

b_X_train.describe()

from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_regression
from sklearn.svm import SVR
import xgboost as xgb

Bmodel =RandomForestRegressor(random_state=42)

Bmodel.fit(b_X_train,b_y_train)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error
from sklearn.utils import shuffle
from sklearn.ensemble import RandomForestRegressor

# Parameters
train_sizes = np.linspace(0.1, 1.0, 10)
n_repeats = 10  # Number of repetitions for each training size

# Containers
train_loss_mean = []
test_loss_mean = []
train_loss_std = []
test_loss_std = []
n_samples = []

# Loop over training fractions
for frac in train_sizes:
    train_losses = []
    test_losses = []

    for seed in range(n_repeats):
        # Shuffle each time with different seed
        X_shuffled, y_shuffled = shuffle(b_X_train, b_y_train, random_state=seed)

        n_train = int(frac * len(X_shuffled))
        X_sub = X_shuffled[:n_train]
        y_sub = y_shuffled[:n_train]

        model = RandomForestRegressor(random_state=42)
        model.fit(X_sub, y_sub)

        y_sub_pred = model.predict(X_sub)
        y_test_pred = model.predict(b_X_test)

        train_loss = mean_absolute_error(y_sub, y_sub_pred)
        test_loss = mean_absolute_error(b_y_test, y_test_pred)

        train_losses.append(train_loss)
        test_losses.append(test_loss)

    # Aggregate stats
    train_loss_mean.append(np.mean(train_losses))
    test_loss_mean.append(np.mean(test_losses))
    train_loss_std.append(np.std(train_losses))
    test_loss_std.append(np.std(test_losses))
    n_samples.append(n_train)

# Plotting with error bars
plt.figure(figsize=(8, 5))
plt.errorbar(n_samples, train_loss_mean, yerr=train_loss_std, label="Train Loss (MAE)", fmt='-o', capsize=5)
plt.errorbar(n_samples, test_loss_mean, yerr=test_loss_std, label="Test Loss (MAE)", fmt='-s', capsize=5)

plt.xlabel("Number of Training Data Points")
plt.ylabel("Loss (MAE)")
plt.title(f"Train/Test MAE vs. Training Size (±1 std over {n_repeats} splits)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.utils import shuffle

# Parameters
train_sizes = np.linspace(0.1, 1.0, 10)
n_repeats = 10  # Number of random shuffles per train size

train_rmse_mean = []
test_rmse_mean = []
train_rmse_std = []
test_rmse_std = []
n_samples = []

# Loop over training sizes
for frac in train_sizes:
    rmse_train_all = []
    rmse_test_all = []

    for i in range(n_repeats):
        # Shuffle each time
        X_shuffled, y_shuffled = shuffle(b_X_train, b_y_train, random_state=i)
        n_train = int(frac * len(X_shuffled))

        X_sub = X_shuffled[:n_train]
        y_sub = y_shuffled[:n_train]

        model = RandomForestRegressor(random_state=42)
        model.fit(X_sub, y_sub)

        # Predict
        y_sub_pred = model.predict(X_sub)
        y_test_pred = model.predict(b_X_test)

        # Compute RMSE
        rmse_train = np.sqrt(mean_squared_error(y_sub, y_sub_pred))
        rmse_test = np.sqrt(mean_squared_error(b_y_test, y_test_pred))

        rmse_train_all.append(rmse_train)
        rmse_test_all.append(rmse_test)

    # Mean and std for each training size
    train_rmse_mean.append(np.mean(rmse_train_all))
    test_rmse_mean.append(np.mean(rmse_test_all))
    train_rmse_std.append(np.std(rmse_train_all))
    test_rmse_std.append(np.std(rmse_test_all))
    n_samples.append(n_train)

# Plotting
plt.figure(figsize=(8, 5))

plt.errorbar(n_samples, train_rmse_mean, yerr=train_rmse_std, label="Train RMSE", fmt='-o', capsize=5)
plt.errorbar(n_samples, test_rmse_mean, yerr=test_rmse_std, label="Test RMSE", fmt='-s', capsize=5)

plt.xlabel("Number of Training Data Points")
plt.ylabel("RMSE")
plt.title(f"Train/Test RMSE vs. Training Data Size\n(Averaged over {n_repeats} random splits)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.utils import shuffle
from sklearn.ensemble import RandomForestRegressor

# Parameters
train_sizes = np.linspace(0.1, 1.0, 10)
n_repeats = 10  # Number of repetitions for each training size

# Containers
train_loss_mean = []
test_loss_mean = []
train_loss_std = []
test_loss_std = []
n_samples = []

# Loop over training fractions
for frac in train_sizes:
    train_losses = []
    test_losses = []

    for seed in range(n_repeats):
        # Shuffle each time with different seed
        X_shuffled, y_shuffled = shuffle(b_X_train, b_y_train, random_state=seed)

        n_train = int(frac * len(X_shuffled))
        X_sub = X_shuffled[:n_train]
        y_sub = y_shuffled[:n_train]

        model = RandomForestRegressor(random_state=42)
        model.fit(X_sub, y_sub)

        y_sub_pred = model.predict(X_sub)
        y_test_pred = model.predict(b_X_test)

        # Compute MAPE (in fraction)
        train_loss = mean_absolute_percentage_error(y_sub, y_sub_pred)
        test_loss = mean_absolute_percentage_error(b_y_test, y_test_pred)

        train_losses.append(train_loss)
        test_losses.append(test_loss)

    # Aggregate stats and convert to percentage
    train_loss_mean.append(np.mean(train_losses) * 100)
    test_loss_mean.append(np.mean(test_losses) * 100)
    train_loss_std.append(np.std(train_losses) * 100)
    test_loss_std.append(np.std(test_losses) * 100)
    n_samples.append(n_train)

# Plotting with error bars
plt.figure(figsize=(8, 5))
plt.errorbar(n_samples, train_loss_mean, yerr=train_loss_std, label="Train Loss (MAPE %)", fmt='-o', capsize=5)
plt.errorbar(n_samples, test_loss_mean, yerr=test_loss_std, label="Test Loss (MAPE %)", fmt='-s', capsize=5)

plt.xlabel("Number of Training Data Points")
plt.ylabel("Loss (MAPE %)")  # Label updated
plt.title(f"Train/Test MAPE (%) vs. Training Size (±1 std over {n_repeats} splits)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn import metrics
train_data_prediction = Bmodel.predict(b_X_train)
error_score1 = metrics.r2_score(b_y_train, train_data_prediction)
error_score2 = metrics.mean_absolute_error(b_y_train, train_data_prediction)
error_score3 = np.sqrt(metrics.mean_squared_error(b_y_train, train_data_prediction))
print("R squared error : ", error_score1)
print('Mean Absolute Error : ', error_score2)
print('Root Mean Squared Error : ', error_score3)

from sklearn import metrics
test_data_prediction = Bmodel.predict(b_X_test)
error_score1 = metrics.r2_score(b_y_test, test_data_prediction)
error_score2 = metrics.mean_absolute_error(b_y_test, test_data_prediction)
error_score3 = np.sqrt(metrics.mean_squared_error(b_y_test, test_data_prediction))
print("R squared error : ", error_score1)
print('Mean Absolute Error : ', error_score2)
print('Root Mean Squared Error : ', error_score3)

import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator, FormatStrFormatter

importances = Bmodel.feature_importances_
top_indices = np.argsort(importances)[-10:]
top_features = b_X_train.columns[top_indices]
cmap = plt.get_cmap('viridis')

colors = cmap(np.linspace(0, 1, len(top_features)))

top_features = [
    r"$\mathbf{\chi}_{\mathrm{\mathbf{mulliken, mean}}}$",  # meanElectronegativity Mulliken
    r"$\mathbf{\chi}_{\mathrm{\mathbf{pauling, mean}}}$",   # meanElectronegativity Pauling
    r"$\mathbf{\chi}_{\mathrm{\mathbf{pauling, min}}}$",   # minElectronegativity Pauling
    r"$\mathbf{R_{\mathrm{\mathbf{cov, max}}}}$",              # maxCovalent Radius
    r"$\mathbf{\alpha_{\mathrm{\mathbf{thermal, mean}}}}$",               # meanThermal Expansion
    r"$\mathbf{\rho_{\mathrm{\mathbf{mean}}}}$",                 # meanDensity
    r"$\mathbf{R_{\mathrm{\mathbf{atomic, max}}}}$",                # maxAtomic Radius
    r"$\mathbf{R_{\mathrm{\mathbf{cov, mean}}}}$",             # meanCovalent Radius
    r"$\mathbf{EA_{\mathrm{\mathbf{var}}}}$",   # varianceElectron Affinity
    r"$\mathbf{\Delta {\mathbf{H_{\mathrm{\mathbf{f, var}}}}}}$"    # varianceHeat of Formation
]

top_features.reverse()

plt.figure(figsize=(6, 6))
plt.barh(range(len(top_features)), importances[top_indices], color=colors, align='center')
plt.yticks(range(len(top_features)), top_features, fontsize=22, fontweight='bold')
plt.xticks([0.0, 0.1, 0.2, 0.3, 0.4], fontsize=22, fontweight='bold')  # Set exact ticks
plt.xlim(0, 0.4)
plt.xticks(fontsize=22, fontweight='bold')
plt.xlabel('Feature Importance', fontsize=22, fontweight='bold')

plt.show()

# --- 1) Build aligned design matrices that match the model's training schema
def align_X(X, model):
    """
    Return X with the exact columns (and order) the model was trained on.
    Raises a clear error if any required column is missing.
    """
    if hasattr(model, "feature_names_in_"):
        needed = list(model.feature_names_in_)
        missing = [c for c in needed if c not in X.columns]
        if missing:
            raise ValueError(f"Your X is missing columns the model was trained on: {missing}")
        return X[needed]  # exact order
    else:
        # Fallback: assume first n_features_in_ columns (works if you never changed order)
        return X.iloc[:, :model.n_features_in_]

b_X_train_aligned = align_X(b_X_train, Bmodel)
b_X_test_aligned  = align_X(b_X_test,  Bmodel)

# --- 2) Per-tree predictions (use the same model you fitted: Bmodel)
import numpy as np

train_preds_per_tree = np.stack([tree.predict(b_X_train_aligned) for tree in Bmodel.estimators_], axis=0)
test_preds_per_tree  = np.stack([tree.predict(b_X_test_aligned)  for tree in Bmodel.estimators_], axis=0)

y_train_pred = train_preds_per_tree.mean(axis=0)
y_test_pred  = test_preds_per_tree.mean(axis=0)

train_std = train_preds_per_tree.std(axis=0, ddof=1)  # ddof=1 is common for sample std
test_std  = test_preds_per_tree.std(axis=0, ddof=1)

# === Training Data Parity Plot ===
from sklearn import metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt

r2 = r2_score(b_y_train, train_data_prediction)
mae = mean_absolute_error(b_y_train, train_data_prediction)
rmse = np.sqrt(mean_squared_error(b_y_train, train_data_prediction))

plt.xlim(100,240)
plt.ylim(100,240)
plt.errorbar(b_y_train, y_train_pred, yerr=train_std, fmt='o', color='black', ecolor='gray', alpha=0.5, label='±1σ error bars')
plt.plot([100, 240], [100, 240], color='red', linestyle='--', linewidth=2, label='y = x')  # Fixed here

plt.xlabel("Actual Bulk Modulus", fontsize=18, fontweight='bold')
plt.ylabel("Predicted Bulk Modulus", fontsize=18, fontweight='bold')
plt.title("True vs Model Bulk Modulus", fontsize=18, fontweight='bold')

# Show R², MAE, and RMSE as a legend
legend_text = f'R²: {r2:.3f}\nMAE: {mae:.3f}\nRMSE: {rmse:.3f}'
plt.legend([legend_text], loc='upper left', fontsize=16, frameon=True, framealpha=1, edgecolor='black', facecolor='white', borderpad=1, labelspacing=1)

plt.tick_params(axis='both', which='major', labelsize=18, width=2, direction='in', pad=5)
plt.tight_layout()
plt.show()

# === Training Data Parity Plot ===
r2 = r2_score(b_y_test, test_data_prediction)
mae = mean_absolute_error(b_y_test, test_data_prediction)
rmse = np.sqrt(mean_squared_error(b_y_test, test_data_prediction))

plt.xlim(100,240)
plt.ylim(100,240)
plt.errorbar(b_y_test, y_test_pred, yerr=test_std, fmt='o', color='blue', ecolor='lightblue', alpha=0.8, label='±1σ error bars')
plt.plot([100, 240], [100, 240], color='red', linestyle='--', linewidth=2, label='y = x')  # Fixed here

plt.xlabel("Actual Bulk Modulus", fontsize=18, fontweight='bold')
plt.ylabel("Predicted Bulk Modulus", fontsize=18, fontweight='bold')
plt.title("True vs Model Bulk Modulus", fontsize=18, fontweight='bold')

# Show R², MAE, and RMSE as a legend
legend_text = f'R²: {r2:.3f}\nMAE: {mae:.3f}\nRMSE: {rmse:.3f}'
plt.legend([legend_text], loc='upper left', fontsize=16, frameon=True, framealpha=1, edgecolor='black', facecolor='white', borderpad=1, labelspacing=1)

plt.tick_params(axis='both', which='major', labelsize=18, width=2, direction='in', pad=5)
plt.tight_layout()
plt.show()

# ==== SHAP beeswarm for your trained RandomForest (robust to SHAP version) ====
import shap
import numpy as np
import matplotlib.pyplot as plt

# Readable fonts (tweak as needed)
plt.rcParams.update({
    "font.size": 16,
    "axes.labelsize": 16,
    "axes.labelweight": "bold",
    "xtick.labelsize": 16,
    "ytick.labelsize": 16,
    "font.weight": "bold",
})

def shap_beeswarm_for_rf(bmodel, b_X_train, b_X_test, max_display=10, sample_bg=1000, sample_test=2000):
    # 1) Background sample for interventional TreeExplainer
    bg = b_X_train
    if len(b_X_train) > sample_bg:
        bg = b_X_train.sample(sample_bg, random_state=42)

    # 2) Keep evaluation set manageable for plotting speed
    X_eval = b_X_test
    if len(b_X_test) > sample_test:
        X_eval = X_test.sample(sample_test, random_state=42)

    # 3) Build explainer
    explainer = shap.TreeExplainer(
        bmodel,
        data=bg,                              # background distribution
        feature_perturbation="interventional"
    )

    # 4) Try the modern API first; fall back to classic if needed
    try:
        values = explainer(X_eval, check_additivity=False)   # shap.Explanation (new API)
        plt.figure(figsize=(10, 10))
        # NOTE: don't pass a custom 'order=' to avoid version issues
        shap.plots.beeswarm(values, max_display=10, show=True)
        #plt.title("SHAP Beeswarm — Feature impacts on B", fontsize=18, fontweight="bold")
        #plt.tight_layout()
    except Exception:
        # Classic API fallback (older SHAP)
        sv = explainer.shap_values(X_eval)   # returns np.ndarray
        shap.summary_plot(sv, X_eval, plot_type="dot", max_display=max_display, show=True)

# ---- call it with your objects ----
shap_beeswarm_for_rf(Bmodel, b_X_train, b_X_test, max_display=10)

from sklearn.model_selection import cross_val_score
from sklearn.utils import shuffle

cv_list = cross_val_score(RandomForestRegressor(random_state = 42), pd.concat([b_X_test,b_X_train]), pd.concat([b_y_test,b_y_train]), cv=5)
print(np.mean(cv_list))

"""#ML on phase prediction"""

from sklearn.model_selection import train_test_split
phases = impute_knn(phases)

p_X_train, p_X_test, p_y_train, p_y_test = train_test_split(phases.drop(columns=["Alloy ","adjusted_phase"]), phases["adjusted_phase"], test_size = 0.25, random_state = 42)

from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier


phase_model = GradientBoostingClassifier(random_state =42)
phase_model.fit(p_X_train,p_y_train)

from matplotlib.ticker import MaxNLocator, FormatStrFormatter
import matplotlib.pyplot as plt

importances = phase_model.feature_importances_
top_indices = np.argsort(importances)[-10:]
top_features = p_X_train.columns[top_indices]
cmap = plt.get_cmap('viridis')

colors = cmap(np.linspace(0, 1, len(top_features)))

top_features = [
    r"$\mathbf{R_{\mathrm{\mathbf{cov, min}}}}$",  # minCovalent Radius
    r"$\mathbf{R_{\mathrm{\mathbf{cov, var}}}}$",   # varianceCovalent Radius
    r"$\mathbf{Z_{\mathrm{\mathbf{cov, var}}}}$",   # varianceAtomic Number
    r"$\mathbf{APF}$",              # Packing Fraction
    r"$\mathbf{\alpha_{\mathrm{\mathbf{thermal, mean}}}}$",               # meanThermal Expansion
    r"$\mathbf{R_{\mathrm{\mathbf{cov, mean}}}}$",                 # meanCovalent Radius
    r"$\mathbf{a_{\mathrm{\mathbf{var}}}}$",                # varianceLattice Constant
    r"$\mathbf{\Delta {\mathbf{S_{\mathrm{\mathbf{mix}}}}}}$",             # Smix
    r"$\mathbf{T_{\mathrm{\mathbf{melting, mean}}}}$",   # meanMelting Point
    r"$\mathbf{EA_{\mathrm{\mathbf{var}}}}$"    # varianceElectron Affinity
]

top_features.reverse()

plt.figure(figsize=(6, 6))
plt.barh(range(len(top_features)), importances[top_indices], color=colors, align='center')
plt.yticks(range(len(top_features)), top_features, fontsize=22, fontweight='bold')
plt.xticks(fontsize=22, fontweight='bold')
plt.xlabel('Feature Importance', fontsize=22, fontweight='bold')
plt.xticks(fontsize = 15)
ax = plt.gca()
#plt.xlim(0,0.3)

plt.show()

from sklearn.model_selection import cross_val_score

cv_list = cross_val_score(GradientBoostingClassifier(random_state = 42), pd.concat([p_X_test,p_X_train]), pd.concat([p_y_test,p_y_train]), cv=5)
print(np.mean(cv_list))

from sklearn.metrics import accuracy_score
y_pred = phase_model.predict(p_X_test)
accuracy_score(p_y_test, y_pred)

from sklearn.metrics import confusion_matrix

confusion_matrix(p_y_test, y_pred, labels = ["BCC","FCC","BCC+FCC","IM"])

from sklearn.metrics import ConfusionMatrixDisplay

disp = ConfusionMatrixDisplay(confusion_matrix(p_y_test, y_pred, labels = phase_model.classes_, normalize = "true"), display_labels=phase_model.classes_)

disp.plot()

# Set title with bold font
plt.title("Phase Prediction", fontsize=18, fontweight='bold')

# Increase font size and make bold for axis labels
plt.xlabel("Predicted Label", fontsize=16, fontweight='bold')
plt.ylabel("True Label", fontsize=16, fontweight='bold')

# Increase font size and make bold for tick labels
plt.xticks(fontsize=14, fontweight='bold')
plt.yticks(fontsize=14, fontweight='bold')

# Show the plot
plt.show()

"""# **Randorm Forest**"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier


phase_model = RandomForestClassifier(random_state =42)
phase_model.fit(p_X_train,p_y_train)

from matplotlib.ticker import MaxNLocator, FormatStrFormatter
import matplotlib.pyplot as plt

importances = phase_model.feature_importances_
top_indices = np.argsort(importances)[-10:]
top_features = p_X_train.columns[top_indices]
cmap = plt.get_cmap('viridis')

colors = cmap(np.linspace(0, 1, len(top_features)))

top_features = [
    r"$\mathbf{R_{\mathrm{\mathbf{cov, min}}}}$",  # minCovalent Radius
    r"$\mathbf{R_{\mathrm{\mathbf{cov, var}}}}$",   # varianceCovalent Radius
    r"$\mathbf{Z_{\mathrm{\mathbf{cov, var}}}}$",   # varianceAtomic Number
    r"$\mathbf{APF}$",              # Packing Fraction
    r"$\mathbf{\alpha_{\mathrm{\mathbf{thermal, mean}}}}$",               # meanThermal Expansion
    r"$\mathbf{R_{\mathrm{\mathbf{cov, mean}}}}$",                 # meanCovalent Radius
    r"$\mathbf{a_{\mathrm{\mathbf{var}}}}$",                # varianceLattice Constant
    r"$\mathbf{\Delta {\mathbf{S_{\mathrm{\mathbf{mix}}}}}}$",             # Smix
    r"$\mathbf{T_{\mathrm{\mathbf{melting, mean}}}}$",   # meanMelting Point
    r"$\mathbf{EA_{\mathrm{\mathbf{var}}}}$"    # varianceElectron Affinity
]

top_features.reverse()

plt.figure(figsize=(6, 6))
plt.barh(range(len(top_features)), importances[top_indices], color=colors, align='center')
plt.yticks(range(len(top_features)), top_features, fontsize=22, fontweight='bold')
plt.xticks(fontsize=22, fontweight='bold')
plt.xlabel('Feature Importance', fontsize=22, fontweight='bold')
plt.xticks(fontsize = 22)
ax = plt.gca()
#plt.xlim(0,0.3)

plt.show()

from sklearn.model_selection import cross_val_score

cv_list = cross_val_score(RandomForestClassifier(random_state = 42), pd.concat([p_X_test,p_X_train]), pd.concat([p_y_test,p_y_train]), cv=5)
print(np.mean(cv_list))

from sklearn.metrics import accuracy_score
y_pred = phase_model.predict(p_X_test)
accuracy_score(p_y_test, y_pred)

from sklearn.metrics import confusion_matrix

confusion_matrix(p_y_test, y_pred, labels = ["BCC","FCC","BCC+FCC","IM"])

from sklearn.metrics import ConfusionMatrixDisplay

disp = ConfusionMatrixDisplay(confusion_matrix(p_y_test, y_pred, labels = phase_model.classes_, normalize = "true"), display_labels=phase_model.classes_)

disp.plot()

# Set title with bold font
plt.title("Phase Prediction", fontsize=18, fontweight='bold')

# Increase font size and make bold for axis labels
plt.xlabel("Predicted Label", fontsize=16, fontweight='bold')
plt.ylabel("True Label", fontsize=16, fontweight='bold')

# Increase font size and make bold for tick labels
plt.xticks(fontsize=14, fontweight='bold')
plt.yticks(fontsize=14, fontweight='bold')

# Show the plot
plt.show()

"""#Solution Energy Prediction

"""

solution_energies_df = featurize_df(solution_energies_df,"Formula")
solution_energies_df.index = pd.RangeIndex(len(solution_energies_df.index))
solution_energies_df = impute_knn(solution_energies_df)
solution_energies_df.describe()

deltaH_train, deltaH_test = train_test_split(solution_energies_df, test_size = 0.25, random_state = 42)

deltaH_Xtrain = deltaH_train.drop(columns=["Delta H","Formula","Smix","pressure","temperature"])
deltaH_ytrain = deltaH_train["Delta H"]

deltaH_Xtest = deltaH_test.drop(columns=["Delta H","Formula","Smix","pressure","temperature"])
deltaH_ytest = deltaH_test["Delta H"]

# Bin the continuous target "Delta H" into 5 quantiles for stratification
n_bins = 5
binned_deltaH = pd.qcut(solution_energies_df["Delta H"], q=n_bins, duplicates='drop')

# Perform stratified train-test split
deltaH_train, deltaH_test = train_test_split(
    solution_energies_df,
    test_size=0.25,
    random_state=42,
    stratify=binned_deltaH
)

# Define training and testing features/labels
deltaH_Xtrain = deltaH_train.drop(columns=["Delta H", "Formula", "Smix", "pressure", "temperature"])
deltaH_ytrain = deltaH_train["Delta H"]

deltaH_Xtest = deltaH_test.drop(columns=["Delta H", "Formula", "Smix", "pressure", "temperature"])
deltaH_ytest = deltaH_test["Delta H"]

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 300, 500],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 4, 5],
    'subsample': [0.8, 1.0],
    'max_features': ['sqrt', 'log2']
}

grid_search = GridSearchCV(
    GradientBoostingRegressor(random_state=42),
    param_grid,
    cv=5,
    scoring='neg_mean_squared_error',  # or 'r2', 'neg_mean_absolute_error'
    n_jobs=-1,
    verbose=1
)

grid_search.fit(deltaH_Xtrain, deltaH_ytrain)

Hmodel = grid_search.best_estimator_

# Show the best hyperparameters found
print("Best hyperparameters:")
print(grid_search.best_params_)

# Show the best cross-validated score (neg. MSE, convert to positive MSE or RMSE if needed)
print(f"\nBest cross-validation score (negative MSE): {grid_search.best_score_:.4f}")
print(f"Best cross-validation RMSE: {(-grid_search.best_score_)**0.5:.4f}")

from sklearn.ensemble import GradientBoostingRegressor
Hmodel = GradientBoostingRegressor(learning_rate=0.05, max_depth=3, n_estimators=500, subsample=0.8, random_state = 42)
Hmodel.fit(deltaH_Xtrain,deltaH_ytrain)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error
from sklearn.utils import shuffle
from sklearn.ensemble import GradientBoostingRegressor

# Parameters
train_sizes = np.linspace(0.1, 1.0, 10)
n_repeats = 10  # Number of repetitions for each training size

# Containers
train_loss_mean = []
test_loss_mean = []
train_loss_std = []
test_loss_std = []
n_samples = []

# Loop over training fractions
for frac in train_sizes:
    train_losses = []
    test_losses = []

    for seed in range(n_repeats):
        # Shuffle each time with different seed
        X_shuffled, y_shuffled = shuffle(deltaH_Xtrain, deltaH_ytrain, random_state=seed)

        n_train = int(frac * len(X_shuffled))
        X_sub = X_shuffled[:n_train]
        y_sub = y_shuffled[:n_train]

        model = GradientBoostingRegressor(learning_rate=0.05, max_depth=3, n_estimators=500, subsample=0.8, random_state = 42)
        model.fit(X_sub, y_sub)

        y_sub_pred = model.predict(X_sub)
        y_test_pred = model.predict(deltaH_Xtest)

        train_loss = mean_absolute_error(y_sub, y_sub_pred)
        test_loss = mean_absolute_error(deltaH_ytest, y_test_pred)

        train_losses.append(train_loss)
        test_losses.append(test_loss)

    # Aggregate stats
    train_loss_mean.append(np.mean(train_losses))
    test_loss_mean.append(np.mean(test_losses))
    train_loss_std.append(np.std(train_losses))
    test_loss_std.append(np.std(test_losses))
    n_samples.append(n_train)

# Plotting with error bars
plt.figure(figsize=(8, 5))
plt.errorbar(n_samples, train_loss_mean, yerr=train_loss_std, label="Train Loss (MAE)", fmt='-o', capsize=5)
plt.errorbar(n_samples, test_loss_mean, yerr=test_loss_std, label="Test Loss (MAE)", fmt='-s', capsize=5)

plt.xlabel("Number of Training Data Points")
plt.ylabel("Loss (MAE)")
plt.title(f"Train/Test MAE vs. Training Size (±1 std over {n_repeats} splits)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.utils import shuffle

# Parameters
train_sizes = np.linspace(0.1, 1.0, 10)
n_repeats = 10  # Number of random shuffles per train size

train_rmse_mean = []
test_rmse_mean = []
train_rmse_std = []
test_rmse_std = []
n_samples = []

# Loop over training sizes
for frac in train_sizes:
    rmse_train_all = []
    rmse_test_all = []

    for i in range(n_repeats):
        # Shuffle each time
        X_shuffled, y_shuffled = shuffle(deltaH_Xtrain, deltaH_ytrain, random_state=i)
        n_train = int(frac * len(X_shuffled))

        X_sub = X_shuffled[:n_train]
        y_sub = y_shuffled[:n_train]

        model = GradientBoostingRegressor(learning_rate=0.05, max_depth=3, n_estimators=500, subsample=0.8, random_state = 42)
        model.fit(X_sub, y_sub)

        # Predict
        y_sub_pred = model.predict(X_sub)
        y_test_pred = model.predict(deltaH_Xtest)

        # Compute RMSE
        rmse_train = np.sqrt(mean_squared_error(y_sub, y_sub_pred))
        rmse_test = np.sqrt(mean_squared_error(deltaH_ytest, y_test_pred))

        rmse_train_all.append(rmse_train)
        rmse_test_all.append(rmse_test)

    # Mean and std for each training size
    train_rmse_mean.append(np.mean(rmse_train_all))
    test_rmse_mean.append(np.mean(rmse_test_all))
    train_rmse_std.append(np.std(rmse_train_all))
    test_rmse_std.append(np.std(rmse_test_all))
    n_samples.append(n_train)

# Plotting
plt.figure(figsize=(8, 5))

plt.errorbar(n_samples, train_rmse_mean, yerr=train_rmse_std, label="Train RMSE", fmt='-o', capsize=5)
plt.errorbar(n_samples, test_rmse_mean, yerr=test_rmse_std, label="Test RMSE", fmt='-s', capsize=5)

plt.xlabel("Number of Training Data Points")
plt.ylabel("RMSE")
plt.title(f"Train/Test RMSE vs. Training Data Size\n(Averaged over {n_repeats} random splits)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.utils import shuffle
from sklearn.ensemble import RandomForestRegressor

# Parameters
train_sizes = np.linspace(0.1, 1.0, 10)
n_repeats = 10  # Number of repetitions for each training size

# Containers
train_loss_mean = []
test_loss_mean = []
train_loss_std = []
test_loss_std = []
n_samples = []

# Loop over training fractions
for frac in train_sizes:
    train_losses = []
    test_losses = []

    for seed in range(n_repeats):
        # Shuffle each time with different seed
        X_shuffled, y_shuffled = shuffle(deltaH_Xtrain, deltaH_ytrain, random_state=seed)

        n_train = int(frac * len(X_shuffled))
        X_sub = X_shuffled[:n_train]
        y_sub = y_shuffled[:n_train]

        model = GradientBoostingRegressor(learning_rate=0.05, max_depth=3, n_estimators=500, subsample=0.8, random_state = 42)
        model.fit(X_sub, y_sub)

        y_sub_pred = model.predict(X_sub)
        y_test_pred = model.predict(deltaH_Xtest)

        # Compute MAPE (in fraction)
        train_loss = mean_absolute_percentage_error(y_sub, y_sub_pred)
        test_loss = mean_absolute_percentage_error(deltaH_ytest, y_test_pred)

        train_losses.append(train_loss)
        test_losses.append(test_loss)

    # Aggregate stats and convert to percentage
    train_loss_mean.append(np.mean(train_losses) * 100)
    test_loss_mean.append(np.mean(test_losses) * 100)
    train_loss_std.append(np.std(train_losses) * 100)
    test_loss_std.append(np.std(test_losses) * 100)
    n_samples.append(n_train)

# Plotting with error bars
plt.figure(figsize=(8, 5))
plt.errorbar(n_samples, train_loss_mean, yerr=train_loss_std, label="Train Loss (MAPE %)", fmt='-o', capsize=5)
plt.errorbar(n_samples, test_loss_mean, yerr=test_loss_std, label="Test Loss (MAPE %)", fmt='-s', capsize=5)

plt.xlabel("Number of Training Data Points")
plt.ylabel("Loss (MAPE %)")  # Label updated
plt.title(f"Train/Test MAPE (%) vs. Training Size (±1 std over {n_repeats} splits)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator, FormatStrFormatter

importances = Hmodel.feature_importances_
top_indices = np.argsort(importances)[-10:]
top_features = deltaH_Xtrain.columns[top_indices]
cmap = plt.get_cmap('viridis')

colors = cmap(np.linspace(0, 1, len(top_features)))

top_features = [
    r"$\mathbf{R_{\mathrm{\mathbf{cov, mean}}}}$",  # meanCovalent Radius
    r"$\mathbf{\chi}_{\mathrm{\mathbf{pauling, mean}}}$",   # meanElectronegativity Pauling
    r"$\mathbf{EA_{\mathrm{\mathbf{min}}}}$",   # minElectron Affinity
    r"$\mathbf{\chi}_{\mathrm{\mathbf{mulliken, var}}}$",              # varianceElectronegativity Mulliken
    r"$\mathbf{R_{\mathrm{\mathbf{atomic, mean}}}}$",               # meanAtomic Radius
    r"$\mathbf{EA_{\mathrm{\mathbf{max}}}}$",                 # maxElectron Affinity
    r"$\mathbf{C_{\mathrm{\mathbf{p, var}}}}$",                # varianceSpecific Heat Capacity
    r"$\mathbf{a_{\mathrm{\mathbf{mean}}}}$",             # meanLattice Constant
    r"$\mathbf{\alpha_{\mathrm{\mathbf{thermal, min}}}}$",   # minThermal Expansion
    r"$\mathbf{\rho_{\mathrm{\mathbf{var}}}}$"    # varianceDensity
]

top_features.reverse()

plt.figure(figsize=(6, 6))
plt.barh(range(len(top_features)), importances[top_indices], color=colors, align='center')
plt.yticks(range(len(top_features)), top_features, fontsize=22, fontweight='bold')
plt.xticks([0.0, 0.1, 0.2, 0.3, 0.4], fontsize=22, fontweight='bold')  # Set exact ticks
plt.xlim(0, 0.4)
plt.xlabel('Feature Importance', fontsize=22, fontweight='bold')
ax = plt.gca()

#plt.xlim(0,0.3)

plt.show()

from sklearn import metrics
train_data_prediction = Hmodel.predict(deltaH_Xtrain)
error_score1 = metrics.r2_score(deltaH_ytrain, train_data_prediction)
error_score2 = metrics.mean_absolute_error(deltaH_ytrain, train_data_prediction)
error_score3 = np.sqrt(metrics.mean_squared_error(deltaH_ytrain, train_data_prediction))
print("R squared error : ", error_score1)
print('Mean Absolute Error : ', error_score2)
print('Root Mean Squared Error : ', error_score3)

from sklearn import metrics
test_data_prediction = Hmodel.predict(deltaH_Xtest)
error_score1 = metrics.r2_score(deltaH_ytest, test_data_prediction)
error_score2 = metrics.mean_absolute_error(deltaH_ytest, test_data_prediction)
error_score3 = np.sqrt(metrics.mean_squared_error(deltaH_ytest, test_data_prediction))
print("R squared error : ", error_score1)
print('Mean Absolute Error : ', error_score2)
print('Root Mean Squared Error : ', error_score3)

import shap
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

plt.rcParams.update({
    "font.size": 16,            # base font size (affects legends/titles)
    "axes.labelsize": 16,       # axis label size
    "axes.labelweight": "bold", # axis label bold
    "xtick.labelsize": 16,
    "ytick.labelsize": 16,
    "font.weight": "bold",
})

explainer = shap.TreeExplainer(
    Hmodel,
    feature_perturbation="interventional",
    data=deltaH_Xtrain
)

shap_test = explainer(deltaH_Xtest)

plt.figure(figsize=(14, 12))
shap.plots.beeswarm(shap_test, max_display=10, show=False)

# Grab axes *after* SHAP draws
ax = plt.gca()

for tick in ax.get_xticklabels() + ax.get_yticklabels():
    tick.set_fontsize(16)
    tick.set_fontweight("bold")

ax.set_xlabel(ax.get_xlabel(), fontsize=16, fontweight="bold")
ax.set_ylabel(ax.get_ylabel(), fontsize=16, fontweight="bold")

mean_abs_shap = np.abs(shap_test.values).mean(axis=0)
rank_df = pd.DataFrame({
    "feature": deltaH_Xtest.columns,
    "mean(|SHAP|)": mean_abs_shap
}).sort_values("mean(|SHAP|)", ascending=False).reset_index(drop=True)

# -----------------------------
topk = rank_df["feature"].head(5).tolist()

pretty_labels = {
    "meanCovalentRadius": r"$\mathbf{R_{\mathrm{\mathbf{cov, mean}}}}$",
    "meanElectronegativity_Pauling": r"$\mathbf{\chi}_{\mathrm{\mathbf{pauling, mean}}}$",
    "minElectronAffinity": r"$\mathbf{EA_{\mathrm{\mathbf{min}}}}$",
    "varElectronegativity_Mulliken": r"$\mathbf{\chi}_{\mathrm{\mathbf{mulliken, var}}}$",
    "meanAtomicRadius": r"$\mathbf{R_{\mathrm{\mathbf{atomic, mean}}}}$",
    "maxElectronAffinity": r"$\mathbf{EA_{\mathrm{\mathbf{max}}}}$",
    "varSpecificHeatCapacity": r"$\mathbf{C_{\mathrm{\mathbf{p, var}}}}$",
    "meanLatticeConstant": r"$\mathbf{a_{\mathrm{\mathbf{mean}}}}$",
    "minThermalExpansion": r"$\mathbf{\alpha_{\mathrm{\mathbf{thermal, min}}}}$",
    "varDensity": r"$\mathbf{\rho_{\mathrm{\mathbf{var}}}}$"
}

# Labeled copy for nicer plot labels (doesn't mutate your original data)
Xtest_pretty = deltaH_Xtest.rename(columns=lambda c: pretty_labels.get(c, c))

# Rewrap Explanation so SHAP uses pretty labels in plots
shap_test_pretty = shap.Explanation(
    values=shap_test.values,
    base_values=shap_test.base_values,
    data=Xtest_pretty.values,
    feature_names=(
        Xtest_pretty.columns if hasattr(Xtest_pretty, "columns") else None
    )
)

def predict_each_stage(X, gbr_model):
    n_estimators = gbr_model.n_estimators
    learning_rate = gbr_model.learning_rate
    init_prediction = gbr_model.init_.predict(X)  # Initial prediction (mean of target)

    # Convert to 1D if needed
    if len(init_prediction.shape) == 2 and init_prediction.shape[1] == 1:
        init_prediction = init_prediction.ravel()

    stage_preds = []
    y_pred = init_prediction.copy()

    for i in range(n_estimators):
        tree = gbr_model.estimators_[i, 0]  # Only one tree per stage for regression
        y_pred += learning_rate * tree.predict(X)
        stage_preds.append(y_pred.copy())  # Save prediction after this stage

    return np.array(stage_preds)  # Shape: (n_stages, n_samples)

stage_predictions_train_xg = predict_each_stage(deltaH_Xtrain, Hmodel)
stage_predictions_test_xg = predict_each_stage(deltaH_Xtest, Hmodel)

# Mean prediction
y_train_pred = np.mean(stage_predictions_train_xg, axis=0)
y_test_pred = np.mean(stage_predictions_test_xg, axis=0)

# Standard deviation as uncertainty
train_std = np.std(stage_predictions_train_xg, axis=0)
test_std = np.std(stage_predictions_test_xg, axis=0)

# === Training Data Parity Plot ===
r2 = r2_score(deltaH_ytrain, train_data_prediction)
mae = mean_absolute_error(deltaH_ytrain, train_data_prediction)
rmse = np.sqrt(mean_squared_error(deltaH_ytrain, train_data_prediction))

plt.xlim(0, 200)
plt.ylim(0, 200)
plt.errorbar(deltaH_ytrain, y_train_pred, yerr=train_std, fmt='o', color='black', ecolor='gray', alpha=0.6, label='±1σ error bars')
plt.plot([0, 200], [0, 200], color='red', linestyle='--', linewidth=2, label='y = x')  # Fixed here

plt.xlabel(r"True $\mathbf{\Delta H}$ (kJ/mol $\mathbf{H_2}$)", fontsize=18, fontweight='bold')
plt.ylabel(r"Model $\mathbf{\Delta H}$ (kJ/mol $\mathbf{H_2}$)", fontsize=18, fontweight='bold')
plt.title(r"True vs Model $\mathbf{\Delta H}$ (kJ/mol $\mathbf{H_2}$)", fontsize=18, fontweight='bold')

legend_text = f'R²: {r2:.3f}\nMAE: {mae:.3f}\nRMSE: {rmse:.3f}'
plt.legend([legend_text], loc='upper left', fontsize=16, frameon=True, framealpha=1, edgecolor='black', facecolor='white', borderpad=1, labelspacing=1)

plt.tick_params(axis='both', which='major', labelsize=18, width=2, direction='in', pad=5)
plt.tight_layout()
plt.show()

# === Training Data Parity Plot ===
r2 = r2_score(deltaH_ytest, test_data_prediction)
mae = mean_absolute_error(deltaH_ytest, test_data_prediction)
rmse = np.sqrt(mean_squared_error(deltaH_ytest, test_data_prediction))

plt.xlim(0, 200)
plt.ylim(0, 200)
plt.errorbar(deltaH_ytest, y_test_pred, yerr=test_std, fmt='o', color='blue', ecolor='lightblue', alpha=0.8, label='±1σ error bars')
plt.plot([0, 200], [0, 200], color='red', linestyle='--', linewidth=2, label='y = x')  # Fixed here

plt.xlabel(r"True $\mathbf{\Delta H}$ (kJ/mol $\mathbf{H_2}$)", fontsize=18, fontweight='bold')
plt.ylabel(r"Model $\mathbf{\Delta H}$ (kJ/mol $\mathbf{H_2}$)", fontsize=18, fontweight='bold')
plt.title(r"True vs Model $\mathbf{\Delta H}$ (kJ/mol $\mathbf{H_2}$)", fontsize=18, fontweight='bold')

legend_text = f'R2: {r2:.3f}\nMAE: {mae:.3f}\nRMSE: {rmse:.3f}'
plt.legend([legend_text], loc='upper left', fontsize=15, frameon=True, framealpha=1, edgecolor='black', facecolor='white', borderpad=1, labelspacing=1)
plt.tick_params(axis='both', which='major', labelsize=16, width=2, direction='in', pad=5)
plt.show()

from sklearn.model_selection import cross_val_score



cv_list = cross_val_score(GradientBoostingRegressor(random_state = 42), pd.concat([deltaH_Xtest,deltaH_Xtrain]),pd.concat([deltaH_ytest,deltaH_ytrain]), cv=5)
print(np.mean(cv_list))

"""#HtoM Preprocessing"""

predicted_phases = phase_model.predict_proba(impute_knn(ss_dataset.drop(columns=["composition_formula","hydrogen_weight_percent","heat_of_formation","pressure","temperature","HtoM","phase"])[p_X_train.columns]))
predicted_B = Bmodel.predict(impute_knn(ss_dataset.drop(columns=["composition_formula","hydrogen_weight_percent","heat_of_formation","pressure","temperature","HtoM","phase"]))[b_X_train.columns])

bcc_chance = []
fcc_chance = []


for p in predicted_phases:
  bcc_chance.append(p[0])
  fcc_chance.append(p[1])

ss_dataset["BCC"] = bcc_chance
ss_dataset["FCC"] = fcc_chance
ss_dataset["Bulk Modulus"] = predicted_B

newhtom = []


for i in range(0,len(ss_dataset["HtoM"])):
  if np.isnan(ss_dataset["HtoM"][i]):
    w= (ss_dataset["hydrogen_weight_percent"][i])/100
    newhtom.append(ss_dataset["meanAtomic Weight"][i]*w/(1-w))
  else:
    newhtom.append(ss_dataset["HtoM"][i])

ss_dataset["newhtom"] = newhtom

from sklearn.utils import shuffle

hydrogen = ss_dataset.drop(columns= ["heat_of_formation","HtoM","hydrogen_weight_percent","composition_formula","meanAtomic Weight","varianceAtomic Weight","minAtomic Weight","maxAtomic Weight","phase","temperature","pressure"])

hydrogen = impute_knn(hydrogen)
hydrogen = shuffle(hydrogen, random_state = 42)


h2_X = hydrogen.drop(columns= "newhtom")
h2_y = hydrogen["newhtom"]

h2_X = shuffle(h2_X, random_state=42)
h2_y = h2_y[h2_X.index]

h2_X.describe()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(h2_X, h2_y, test_size = 0.25, random_state = 42)

"""#HtoM Model

"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_regression
from sklearn.svm import SVR
import xgboost as xgb

# model =xgb.XGBRegressor(random_state = 42)
model = RandomForestRegressor(random_state = 42)
model.fit(X_train,y_train)

from sklearn import metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
training_data_prediction = model.predict(X_train)
score_1 = metrics.r2_score(y_train, training_data_prediction)

score_2 = metrics.mean_absolute_error(y_train, training_data_prediction)

score_3 = np.sqrt(metrics.mean_squared_error(y_train, training_data_prediction))

print("R squared error : ", score_1)
print('Mean Absolute Error : ', score_2)
print('Root Mean Squared Error : ', score_3)

from sklearn import metrics
test_data_prediction = model.predict(X_test)
error_score1 = metrics.r2_score(y_test, test_data_prediction)
error_score2 = metrics.mean_absolute_error(y_test, test_data_prediction)
error_score3 = np.sqrt(metrics.mean_squared_error(y_test, test_data_prediction))
print("R squared error : ", error_score1)
print('Mean Absolute Error : ', error_score2)
print('Root Mean Squared Error : ', error_score3)

import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator, FormatStrFormatter

importances = model.feature_importances_
top_indices = np.argsort(importances)[-10:]
top_features = X_train.columns[top_indices]
cmap = plt.get_cmap('viridis')

colors = cmap(np.linspace(0, 1, len(top_features)))

top_features = [
    r"$\mathbf{\chi}_{\mathrm{\mathbf{mulliken, mean}}}$",  # meanElectronegativity Mulliken
    r"$\mathbf{T_{\mathrm{\mathbf{melting, mean}}}}$",   # meanMelting Point
    r"$\mathbf{\alpha_{\mathrm{\mathbf{thermal, mean}}}}$",   # meanThermal Expansion
    r"$\mathbf{\alpha_{\mathrm{\mathbf{thermal, var}}}}$",   # varianceThermal Expansion
    r"$\mathbf{K}$",                                           # Bulk Modulus
    r"$\mathbf{\Delta{H_{\mathrm{\mathbf{f, mean}}}}}$",                 # meanHeat of formation
    r"$\mathbf{R_{\mathrm{\mathbf{cov, mean}}}}$",                # meanCovalent Radius
    r"$\mathbf{EA_{\mathrm{\mathbf{mean}}}}$",             # minElectron Affinity
    r"$\mathbf{\chi}_{\mathrm{\mathbf{mulliken, var}}}$",   # varianceElectronegativity Mulliken
    r"$\mathbf{\chi}_{\mathrm{\mathbf{pauling, mean}}}$"    # meanElectronegativity Pauling
]

top_features.reverse()

plt.figure(figsize=(6, 6))

plt.barh(range(len(top_features)), importances[top_indices], color=colors, align='center')
plt.yticks(range(len(top_features)), top_features, fontsize=22, fontweight='bold')
plt.xticks([0.0, 0.1, 0.2, 0.3], fontsize=22, fontweight='bold')  # Set exact ticks
plt.xlim(0, 0.3)
plt.xlabel('Feature Importance', fontsize=22, fontweight='bold')
ax = plt.gca()

#plt.xlim(0,0.3)

plt.show()

# ==== SHAP beeswarm (only) with big, bold labels ====
import shap
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

plt.rcParams.update({
    "font.size": 16,            # base font size
    "axes.labelsize": 16,       # axis label size
    "axes.labelweight": "bold", # axis label bold
    "xtick.labelsize": 16,      # x tick font size
    "ytick.labelsize": 16,      # y tick font size
    "font.weight": "bold",
})

explainer = shap.TreeExplainer(
    model,
    feature_perturbation="interventional",
    data=X_train
)
shap_exp = explainer(X_test, check_additivity=False)
plt.figure(figsize=(14, 12))
shap.plots.beeswarm(shap_exp, max_display=10, show=False)
ax = plt.gca()

for tick in ax.get_xticklabels() + ax.get_yticklabels():
    tick.set_fontsize(16)
    tick.set_fontweight("bold")

ax.set_xlabel("SHAP value (impact on model output)", fontsize=18, fontweight="bold")
#ax.set_ylabel("Feature", fontsize=20, fontweight="bold")

X_pretty = X_test.rename(columns=lambda c: pretty_labels.get(c, c))

shap_exp_pretty = shap.Explanation(
    values=shap_exp.values,
    base_values=shap_exp.base_values,
    data=X_pretty.values,
    feature_names=list(X_pretty.columns)
)

train_preds_per_tree = np.stack([tree.predict(X_train) for tree in model.estimators_])
test_preds_per_tree = np.stack([tree.predict(X_test) for tree in model.estimators_])

y_train_pred = np.mean(train_preds_per_tree, axis=0)
y_test_pred = np.mean(test_preds_per_tree, axis=0)

train_std = np.std(train_preds_per_tree, axis=0)
test_std = np.std(test_preds_per_tree, axis=0)

# === Training Data Parity Plot ===
r2 = r2_score(y_train, training_data_prediction)
mae = mean_absolute_error(y_train, training_data_prediction)
rmse = np.sqrt(mean_squared_error(y_train, training_data_prediction))

plt.xlim(0,2.5)
plt.ylim(0,2.5)
plt.errorbar(y_train, y_train_pred, yerr=train_std, fmt='o', color='black', ecolor='gray', alpha=0.7, label='±1σ error bars')
plt.plot([0, 2.5], [0, 2.5], color='red', linestyle='--', linewidth=2, label='y = x')  # Fixed here

plt.xlabel("True HtoM", fontsize=18, fontweight='bold')
plt.ylabel("Model HtoM", fontsize=18, fontweight='bold')
plt.title(" True vs Model HtoM", fontsize=18, fontweight='bold')

# Show R², MAE, and RMSE as a legend
legend_text = f'R²: {r2:.3f}\nMAE: {mae:.3f}\nRMSE: {rmse:.3f}'
plt.legend([legend_text], loc='upper left', fontsize=16, frameon=True, framealpha=1, edgecolor='black', facecolor='white', borderpad=1, labelspacing=1)
plt.tick_params(axis='both', which='major', labelsize=18, width=2, direction='in', pad=5)
plt.show()

# === Training Data Parity Plot ===
r2 = r2_score(y_test, test_data_prediction)
mae = mean_absolute_error(y_test, test_data_prediction)
rmse = np.sqrt(mean_squared_error(y_test, test_data_prediction))

plt.xlim(0,2.5)
plt.ylim(0,2.5)
plt.errorbar(y_test, y_test_pred, yerr=test_std, fmt='o', color='blue', ecolor='lightblue', alpha=0.7, label='±1σ error bars')
plt.plot([0, 2.5], [0, 2.5], color='red', linestyle='--', linewidth=2, label='y = x')  # Fixed here

plt.xlabel("True HtoM", fontsize=18, fontweight='bold')
plt.ylabel("Model HtoM", fontsize=18, fontweight='bold')
plt.title(" True vs Model HtoM", fontsize=18, fontweight='bold')

# Show R², MAE, and RMSE as a legend
legend_text = f'R²: {r2:.3f}\nMAE: {mae:.3f}\nRMSE: {rmse:.3f}'
plt.legend([legend_text], loc='upper left', fontsize=16, frameon=True, framealpha=1, edgecolor='black', facecolor='white', borderpad=1, labelspacing=1)
plt.tick_params(axis='both', which='major', labelsize=18, width=2, direction='in', pad=5)
plt.show()

from sklearn.model_selection import cross_val_score

h2_X = shuffle(h2_X, random_state=42)
h2_y = h2_y[h2_X.index]

cv_list = cross_val_score(RandomForestRegressor(random_state = 42), h2_X, h2_y, cv=4)
print(np.mean(cv_list))

cv_list

"""#MoNbTi Prediction

"""

def predict_compositions(compositions, temperature, pressure):
  df = pd.DataFrame(columns=["composition_formula"])
  df["composition_formula"] = compositions

  df = featurize_df(df,"composition_formula")
  df = df.drop(columns = ["composition_formula"])


  predicted_solutions = Hmodel.predict(df[deltaH_Xtrain.columns])


  predicted_phases = phase_model.predict_proba(df[p_X_train.columns])
  bulk_moduli = Bmodel.predict(df[b_X_train.columns])

  bcc_chance = []
  fcc_chance = []

  atomic_weights =  df["meanAtomic Weight"]
  df = df.drop(columns= ["meanAtomic Weight","varianceAtomic Weight","minAtomic Weight","maxAtomic Weight"])
  for p in predicted_phases:
    bcc_chance.append(p[0])
    fcc_chance.append(p[1])

  df["BCC"] = bcc_chance
  df["FCC"] = fcc_chance
  df["Bulk Modulus"] = bulk_moduli



  return model.predict(df), bcc_chance, atomic_weights, predicted_solutions, fcc_chance

def ternary_df_maker(s1, s2, s3, pressure =1, temperature =20):
  mos = []
  nbs = []
  tis = []
  monbti = []

  for i in range(0,21):
    for j in range(0, 21-i):
      k = 20 - i - j

      if i == j == k == 0:
        continue
      else:
        ns1 = str(i*5)
        ns2 = str(j*5)
        ns3 = str(k*5)

        ss1 = s1
        ss2 = s2
        ss3 = s3

        if i == 0:
          ss1 = ""
          ns1 = ""
        if j == 0:
          ss2 = ""
          ns2 = ""
        if k == 0:
          ss3 = ""
          ns3 = ""

        monbti.append(ss1+ns1+ss2+ns2+ss3+ns3)

        mos.append(i*5)
        nbs.append(j*5)
        tis.append(k*5)

  htoms, bcc_chances, weights, solutions, fcc_chances = predict_compositions(monbti, temperature, pressure)
  print(len(htoms))
  print(len(monbti))
  return pd.DataFrame({"compositions":monbti,"HtoM":htoms, "Solutions": solutions,"BCC_prob":bcc_chances, "Mo":mos, "Nb":nbs,"Ti":tis, "Weights":weights})

monbti_1_atm = ternary_df_maker("Mg","Ti","V", pressure = 1, temperature = 20)

def plottern(t,l,r,v1, name):

  #HtoM
  fig = plt.figure(figsize=(10.8, 4.8))
  fig.subplots_adjust(left=0.075, right=0.85, wspace=0.3)


  v= np.array(v1)
  ax = fig.add_subplot(1, 2, 2, projection='ternary')
  cs = ax.tripcolor(
      t, l, r, v, shading='gouraud',cmap = "Spectral")
  contours = ax.tricontour(t, l, r, v, colors="k", linewidths=0.5)
  ax.set_title(name, fontweight = "bold")
  ax.set_tlabel(name[0:2])
  ax.set_llabel(name[3:5])
  ax.set_rlabel(name[6:8])
  ax.scatter(first, second, third, c = "white", marker = "*", s=50)
  ax.clabel(contours, contours.levels)
  cax = ax.inset_axes([1.05, 0.1, 0.05, 0.9], transform=ax.transAxes)
  colorbar = fig.colorbar(cs, cax=cax)
  colorbar.set_label(name[8:], rotation=270, va='baseline')

plottern(monbti_1_atm["Mo"],monbti_1_atm["Nb"],monbti_1_atm["Ti"],monbti_1_atm["HtoM"]/monbti_1_atm["Weights"]*100,"Mg-Ti-V Weight Percent")

best_each_round